{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8oy6_b5tIIe",
        "outputId": "5a48501a-afba-45e3-96e0-997d6f7f4d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5vk-M59iHp",
        "outputId": "ea067a44-b625-46e8-be7f-b72c0410c251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vD8WBkO9pVT",
        "outputId": "d456ef01-d4c8-4860-a18c-588214c68d7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-BX8uU9ycu",
        "outputId": "6c31a381-bb10-4403-c0eb-414c7cad0328"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "daxjZD5c9zqo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfGsGxvm940t",
        "outputId": "4438dca0-75b2-4b67-d15a-9ed563244924"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izi-7h2799z2",
        "outputId": "ba2923ba-3b8b-4143-8c84-2e2e0952f28b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "RsskZmpO-Chi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "HidbVnPy-U03"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "NgjdIHfA-bPI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_EK48Q7-fU0",
        "outputId": "2362f6d8-0cfa-4030-bed7-e433dcd46f8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "mEXlzrqg-je6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAS9PUc1-pSk",
        "outputId": "4c949170-0195-4ff6-c5a0-4827c6ce19db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cV6eije-uM1",
        "outputId": "0b75fe5c-a433-4197-ed6a-54d41202f4bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5syNWLKl-0du",
        "outputId": "91e1c6a4-b39c-4c3e-b5ff-40ff782df4de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.85024634e-03  1.65567698e-03 -1.81738939e-03 ... -2.44658417e-03\n",
            "   -2.74311984e-03 -4.65289317e-03]\n",
            "  [-3.01057613e-03  3.14018317e-03 -1.48372899e-04 ... -1.78218016e-03\n",
            "   -1.03205140e-03 -5.00329956e-03]\n",
            "  [ 6.39770122e-04 -2.39068642e-04 -9.00931284e-03 ...  1.70725095e-03\n",
            "   -1.16185925e-03  1.68139185e-03]\n",
            "  ...\n",
            "  [ 1.09125802e-03  5.71241090e-03 -4.59420355e-03 ... -8.81332438e-04\n",
            "   -1.40497107e-02  1.13486983e-02]\n",
            "  [ 3.67151666e-03  4.18187073e-03  3.95439006e-03 ...  7.50035699e-03\n",
            "   -2.94375629e-03  1.04372045e-02]\n",
            "  [ 8.05591932e-04  4.68742289e-03  2.69060070e-03 ...  6.62863674e-03\n",
            "   -5.25720092e-03  6.07130025e-03]]\n",
            "\n",
            " [[-6.07479876e-03  6.85411552e-03 -1.74195040e-04 ... -2.23765220e-03\n",
            "   -3.45840096e-03 -4.12077177e-03]\n",
            "  [-6.18601963e-03  6.62945397e-03  2.09354120e-03 ... -1.95917767e-03\n",
            "   -1.77946640e-03 -4.56039049e-03]\n",
            "  [-6.66042930e-03  4.90491930e-03  1.71975873e-03 ...  1.55093381e-03\n",
            "   -3.80370766e-03 -3.11958371e-03]\n",
            "  ...\n",
            "  [-3.40980012e-03  6.78257085e-04  1.02396971e-02 ...  4.01670253e-03\n",
            "    2.12730328e-03 -2.48285406e-03]\n",
            "  [-3.58785200e-03  3.65066156e-03  6.75891433e-03 ... -5.72241843e-04\n",
            "    1.83837349e-03  1.32032437e-06]\n",
            "  [-5.32076089e-03  4.03394224e-03  6.56544929e-03 ... -3.47047811e-04\n",
            "    2.54490785e-03 -2.08483264e-03]]\n",
            "\n",
            " [[-3.54777789e-03  3.71197052e-03  6.53637922e-04 ... -1.39592553e-03\n",
            "   -2.89609935e-03 -8.09897855e-03]\n",
            "  [-5.14674466e-03 -5.40261855e-04  6.50389446e-03 ... -4.58574342e-03\n",
            "   -6.26218831e-03 -1.68377894e-03]\n",
            "  [-6.83705276e-03 -7.63992080e-04  3.95346619e-03 ...  1.23066630e-03\n",
            "   -6.67770719e-03 -1.03802723e-03]\n",
            "  ...\n",
            "  [-1.36335078e-03  6.99514057e-03  2.76424177e-03 ... -1.23872086e-02\n",
            "   -1.69546856e-03 -3.09574651e-03]\n",
            "  [-1.88073260e-03  6.29038457e-03  2.51294207e-03 ... -5.72450506e-03\n",
            "   -3.56904953e-03 -2.64026993e-03]\n",
            "  [-3.95675143e-03  1.13426056e-03  9.04280320e-03 ... -8.17586761e-03\n",
            "   -5.97253721e-03  1.72373094e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.33510493e-03  1.73047301e-03  1.36800029e-03 ...  2.42975540e-04\n",
            "    9.31966701e-04 -1.44854269e-03]\n",
            "  [-4.73908009e-03  4.98784799e-03  1.95911480e-03 ... -1.37855951e-03\n",
            "   -2.15371512e-03 -8.50488991e-03]\n",
            "  [-3.59900249e-03  8.09877552e-03  6.49543596e-04 ... -4.89806384e-03\n",
            "   -1.71994069e-03 -3.11604585e-03]\n",
            "  ...\n",
            "  [-1.40271643e-02  1.07398015e-02  5.47677185e-03 ... -2.42805341e-03\n",
            "   -7.16703711e-03 -1.37919681e-02]\n",
            "  [-9.17558372e-03  1.19556114e-02  8.42808560e-03 ... -6.45130640e-03\n",
            "   -1.16537209e-04 -9.99686122e-03]\n",
            "  [-7.58307986e-03  1.10660745e-02  4.89671435e-03 ... -8.30721483e-03\n",
            "   -2.05507455e-03 -1.14231324e-02]]\n",
            "\n",
            " [[-2.89285788e-03 -1.74038007e-03  1.18144474e-03 ...  2.97396258e-03\n",
            "    2.14007427e-03 -2.17893999e-03]\n",
            "  [-3.93580738e-03  1.05843483e-03 -1.08727301e-03 ... -5.11229679e-04\n",
            "   -1.43025024e-03 -6.45190571e-03]\n",
            "  [-1.35422219e-03 -8.83962726e-04  4.66644811e-03 ...  3.42632574e-03\n",
            "    5.10462001e-03  3.81069025e-04]\n",
            "  ...\n",
            "  [-1.29664531e-02  5.48327435e-03  2.30499357e-03 ... -2.90666614e-03\n",
            "   -5.96404169e-03 -1.00486428e-02]\n",
            "  [-8.93384870e-03 -1.31952809e-04  8.67000129e-03 ...  1.57047156e-03\n",
            "    3.18436231e-03 -2.63491459e-03]\n",
            "  [-6.39710249e-03  2.45676585e-03  9.96596832e-03 ... -2.84281187e-03\n",
            "    8.43654759e-03 -2.87370454e-03]]\n",
            "\n",
            " [[ 7.56681897e-04 -6.55200472e-03  1.28184143e-03 ...  4.95726056e-03\n",
            "   -2.25062273e-03  2.63177184e-03]\n",
            "  [-3.16118682e-03 -5.44700772e-03 -2.26630713e-03 ...  1.67541858e-03\n",
            "   -3.39430524e-04  2.91358540e-03]\n",
            "  [-3.97446286e-03 -1.05757883e-03 -4.63953428e-03 ...  4.50906344e-03\n",
            "   -8.25692527e-03  2.20085494e-03]\n",
            "  ...\n",
            "  [-8.48599523e-03  1.38299474e-02 -2.43847538e-03 ... -8.17776937e-03\n",
            "   -6.49369787e-03 -1.33177536e-02]\n",
            "  [-8.41260888e-03  8.84106755e-03 -1.56456162e-03 ... -3.52819497e-03\n",
            "   -6.04461692e-03 -1.18610021e-02]\n",
            "  [-5.11447666e-03  2.74590566e-03  5.56088751e-03 ...  8.94103432e-04\n",
            "    3.60825192e-03 -4.53638285e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3NplaMZ-6bW",
        "outputId": "a60d7750-2d8b-4c0f-87df-ce01429c2467"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00185025  0.00165568 -0.00181739 ... -0.00244658 -0.00274312\n",
            "  -0.00465289]\n",
            " [-0.00301058  0.00314018 -0.00014837 ... -0.00178218 -0.00103205\n",
            "  -0.0050033 ]\n",
            " [ 0.00063977 -0.00023907 -0.00900931 ...  0.00170725 -0.00116186\n",
            "   0.00168139]\n",
            " ...\n",
            " [ 0.00109126  0.00571241 -0.0045942  ... -0.00088133 -0.01404971\n",
            "   0.0113487 ]\n",
            " [ 0.00367152  0.00418187  0.00395439 ...  0.00750036 -0.00294376\n",
            "   0.0104372 ]\n",
            " [ 0.00080559  0.00468742  0.0026906  ...  0.00662864 -0.0052572\n",
            "   0.0060713 ]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHrhrA2l-9-t",
        "outputId": "83d213fb-a84e-487e-84c3-d81eb33afcba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.8502463e-03  1.6556770e-03 -1.8173894e-03  3.3495473e-03\n",
            " -2.1389248e-03  5.9776660e-03 -1.1803122e-03 -5.5387453e-03\n",
            " -1.4418662e-03 -8.3111897e-03  3.1458493e-03 -2.4265854e-03\n",
            " -6.0621067e-03 -4.6094409e-03 -8.5900980e-04  7.4464013e-04\n",
            "  8.9333393e-04 -3.0684465e-04 -2.7248282e-03 -8.7916228e-04\n",
            "  4.7804439e-03 -3.3156497e-03 -3.7552756e-03  5.5657438e-04\n",
            "  1.5526661e-04 -6.5634027e-04  1.5822034e-03 -2.3365470e-03\n",
            "  3.6437116e-03  1.4208192e-03  3.2233533e-03 -3.5853623e-03\n",
            "  2.3739694e-03  1.3485546e-03 -2.7355370e-03 -7.8930613e-03\n",
            "  2.2439999e-03  3.3515105e-03 -3.5895230e-03 -1.8390790e-03\n",
            " -6.2095453e-03 -4.1148491e-04 -3.7809990e-03  1.9820258e-03\n",
            "  5.1316514e-05  3.1317929e-03 -9.5966383e-04 -2.6842644e-03\n",
            "  3.2864604e-03 -1.4403842e-03 -9.7981468e-04 -7.7154534e-04\n",
            " -4.0753963e-03  3.0712129e-03  6.5891230e-03  1.9171652e-03\n",
            " -4.4065779e-03 -1.0739077e-03 -1.4960145e-03 -1.7669357e-03\n",
            " -5.0405990e-03  3.4194693e-03 -2.4465842e-03 -2.7431198e-03\n",
            " -4.6528932e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CJ8NdLt__Dug",
        "outputId": "2f48d98e-8906-4628-d194-2e8159bb5a96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IE&dmMbxbEfQ-Bo:WYbtTQ.UIrlCrT;C.XVzkgGfJGSvIcmmsmvikfd bEalx,KRRnG aQv$ RbGIOYcVs,sC3?YS,hB?xRIxnl&'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "bjUEuZO1_H-v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "cKueaCpJ_NqS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "ln9vt_A9_RTG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=1, callbacks=[checkpoint_callback]) #Use a higher number of epochs for better results."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdQs7LUh_WAP",
        "outputId": "f38c1b3e-41c2-4fdf-a352-5689782e1744"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1339s 8s/step - loss: 1.7308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "OuHF0_CtPWwp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "-PLyT9hVPXzY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "yfKkmVn_P2O3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHGCiI7OP6OU",
        "outputId": "e739aeaf-74e9-455f-f918-f1a7c7af61f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: I love natural language processing\n",
            "I love natural language processings.\n",
            "\n",
            "LIONANEL:\n",
            "ULOuld with my:\n",
            "And, what is the knors: waccest the ruent home\n",
            "concullent me have comemity.\n",
            "\n",
            "Suchard: ichinare, bode! so nigh himself, what he\n",
            "jows, nothing be and on frow san was frecturs, theref,\n",
            "Os with meetion and call a Vorsence?\n",
            "Call have say and thot spidition,\n",
            "Too Inten amanies, steak nif\n",
            "Unour dos\n",
            "Hay spike, but fie it pay nol I 'ever her war,\n",
            "I'll oper the tearn'd bear my Greath.\n",
            "\n",
            "Coming Mormar:\n",
            "I lode thos nobthible go?\n",
            "\n",
            "GREUCE:\n",
            "Ame in morery.\n",
            "\n",
            "Lere and seem the doart:\n",
            "'tis forse that had stinkent he prieven:\n",
            "Ancenty mine edy aldy morrisch; but thoreating\n",
            "Tarking me knoparous with at thy manier:\n",
            "Rore hadse I read, but leriesp,\n",
            "Come, to but in spest the greechers steat.\n",
            "\n",
            "Thar IVIND:\n",
            "Seeve love! who his drainst'd nothing will her enverges\n",
            "To his in hat honour brother\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlxOKsCFPgM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVW66tyV_a6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}